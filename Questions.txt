1-Does the robots.txt file need to be saved in a file incase the crawler got interuppted?
2-If the number of crawler instances is changed when the crawler is interuppted and resumed, how to handle? (VIP)